{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Problems in recurrent VGG16 \n",
    "\n",
    "In this notebook, I show some problems I met when I try to recurrent VGG16 performence in CalTech101 \n",
    "\n",
    "\n",
    "performence:\n",
    "![performence in CalTech101](https://www.robots.ox.ac.uk/~vgg/research/very_deep/images/table_other.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training part \n",
    "\n",
    "below are some issues I think may effect accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 1: How to use dataset: CalTech 101\n",
    "\n",
    "In the original text, the description of datatest:\n",
    "\n",
    "In this section, we present the image classification results achieved by the described\n",
    "ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M\n",
    "images), validation (50K images), and testing (100K images with held-out class labels).\n",
    "\n",
    "unfortunately, I do not see any description of CalTech in original paper.\n",
    "\n",
    "But in CalTech website, they said :\n",
    "\n",
    "How to use the dataset\n",
    "If you are using the Caltech 101 dataset for testing your recognition algorithm you should try and make your results comparable to the results of others. We suggest training and testing on fixed number of pictures and repeating the experiment with different random selections of pictures in order to obtain error bars. Popular number of training images: 1, 3, 5, 10, 15, 20, 30. Popular numbers of testing images: 20, 30. See also the discussion below.\n",
    "When you report your results please keep track of which images you used and which were misclassified. We will soon publish a more detailed experimental protocol that allows you to report those details. See the Discussion section for more details.\n",
    "\n",
    "[CalTech 101 collection](http://www.vision.caltech.edu/Image_Datasets/Caltech101/). \n",
    "\n",
    "There are about 40 to 800 images in each category. If we use the ratio in original paper: (26 : 1 : 2), some test sets only have 2 images, that is unreasonable smell. If we follow the seggustion of Caltech website, the traing set may too smell. below is the number of images in some classes when I set the ratio (5, 2.5, 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "traindir = 'data/CalTech101(50, 25, 25)/train/'\n",
    "validdir = 'data/CalTech101(50, 25, 25)/val/'\n",
    "testdir = 'data/CalTech101(50, 25, 25)/test/'\n",
    "# Empty lists\n",
    "categories = []\n",
    "img_categories = []\n",
    "n_train = []\n",
    "n_valid = []\n",
    "n_test = []\n",
    "hs = []\n",
    "ws = []\n",
    "\n",
    "# Iterate through each category\n",
    "for d in os.listdir(traindir):\n",
    "    categories.append(d)\n",
    "\n",
    "    # Number of each image\n",
    "    train_imgs = os.listdir(traindir + d)\n",
    "    valid_imgs = os.listdir(validdir + d)\n",
    "    test_imgs = os.listdir(testdir + d)\n",
    "    n_train.append(len(train_imgs))\n",
    "    n_valid.append(len(valid_imgs))\n",
    "    n_test.append(len(test_imgs))\n",
    "\n",
    "    # Find stats for train images\n",
    "    for i in train_imgs:\n",
    "        img_categories.append(d)\n",
    "        img = Image.open(traindir + d + '/' + i)\n",
    "        img_array = np.array(img)\n",
    "        hs.append(img_array.shape[0])\n",
    "        ws.append(img_array.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "# Dataframe of categories\n",
    "cat_df = pd.DataFrame({'category': categories,\n",
    "                       'n_train': n_train,\n",
    "                       'n_valid': n_valid, 'n_test': n_test}).\\\n",
    "    sort_values('category')\n",
    "# Dataframe of training images\n",
    "image_df = pd.DataFrame({'category': img_categories,'height': hs,'width': ws})\n",
    "\n",
    "cat_df.sort_values('n_train', ascending=False, inplace=True)\n",
    "cat_df.head()\n",
    "\n",
    "cat_df.tail()\n",
    "print (cat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Batch size and iteration \n",
    "\n",
    "\n",
    "\n",
    "In the original text, the description of hypeparameters:\n",
    "\n",
    "The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the L2 penalty multiplier set to 5 · 10−4) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5).\n",
    "The learning rate was initially set to 10−2, and then decreased by a factor of 10 when the validation\n",
    "set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning\n",
    "was stopped after 370K iterations (74 epochs). \n",
    "\n",
    "\n",
    "unfortunately, I cannot set the batch size to 256, because when I set the batch size to 32 or bigger than 32, the GPU will out of memory \n",
    "\n",
    "Because the batch size and number of images is different, the number of iterations will change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: initialisation \n",
    "\n",
    "\n",
    "In the original text, the description of initialisation:\n",
    "\n",
    "The initialisation of the network weights is important, since bad initialisation can stall learning due\n",
    "to the instability of gradient in deep nets. To circumvent this problem, we began with training\n",
    "the configuration A (Table 1), shallow enough to be trained with random initialisation. Then, when\n",
    "training deeper architectures, we initialised the first four convolutional layers and the last three fullyconnected layers with the layers of net A (the intermediate layers were initialised randomly). We did\n",
    "not decrease the learning rate for the pre-initialised layers, allowing them to change during learning.\n",
    "For random initialisation (where applicable), we sampled the weights from a normal distribution\n",
    "with the zero mean and 10−2 variance. The biases were initialised with zero. It is worth noting that\n",
    "after the paper submission we found that it is possible to initialise the weights without pre-training\n",
    "by using the random initialisation procedure of Glorot & Bengio (2010).\n",
    "\n",
    "(The configuration A is VGG11)\n",
    "It is easy to understand the step of random initialisation, But the problem is how to shallow enough pre-train the first four convolutional layers and the last three fullyconnected layers. How to judge it is enough. \n",
    "\n",
    "Previously, I just use the initialisation function from [Pytorch VGG model](https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py). \n",
    "code in below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initialize_weights(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Training image size \n",
    "\n",
    "\n",
    "In the original text, the description of image size:\n",
    "\n",
    "Training image size. Let S be the smallest side of an isotropically-rescaled training image, from\n",
    "which the ConvNet input is cropped (we also refer to S as the training scale). While the crop size\n",
    "is fixed to 224 × 224, in principle S can take on any value not less than 224: for S = 224 the crop\n",
    "will capture whole-image statistics, completely spanning the smallest side of a training image; for\n",
    "S ≫ 224 the crop will correspond to a small part of the image, containing a small object or an object\n",
    "part.\n",
    "We consider two approaches for setting the training scale S. The first is to fix S, which corresponds\n",
    "to single-scale training (note that image content within the sampled crops can still represent multiscale image statistics). In our experiments, we evaluated models trained at two fixed scales: S =\n",
    "256 (which has been widely used in the prior art (Krizhevsky et al., 2012; Zeiler & Fergus, 2013;\n",
    "Sermanet et al., 2014)) and S = 384. Given a ConvNet configuration, we first trained the network\n",
    "using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights\n",
    "pre-trained with S = 256, and we used a smaller initial learning rate of 10−3\n",
    ".\n",
    "The second approach to setting S is multi-scale training, where each training image is individually\n",
    "rescaled by randomly sampling S from a certain range [Smin, Smax] (we used Smin = 256 and\n",
    "Smax = 512). Since objects in images can be of different size, it is beneficial to take this into account\n",
    "during training. This can also be seen as training set augmentation by scale jittering, where a single model is trained to recognise objects over a wide range of scales. For speed reasons, we trained\n",
    "multi-scale models by fine-tuning all layers of a single-scale model with the same configuration,\n",
    "pre-trained with fixed S = 384.\n",
    "\n",
    "Now I understand randomly sampling, but I found the size of images in imagenet and CalTech 101 is different. the images in imagenet are more bigger. \n",
    "\"The average image resolution on ImageNet is 469x387 pixels, normally there is a pre-processing that samples them to 256x256 as @Prune said.\" Maybe not true. I found in [stackoverflow](https://stackoverflow.com/questions/36109886/what-is-the-resolution-of-an-image-in-imagenet-dataset/36191512)\n",
    "\n",
    "And the images size in CalTech 101 in below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dsc = image_df.groupby('category').describe()\n",
    "img_dsc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(\n",
    "    img_dsc['height']['mean'], label='Average Height')\n",
    "sns.kdeplot(\n",
    "    img_dsc['width']['mean'], label='Average Width')\n",
    "plt.xlabel('Pixels')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Average Size Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
